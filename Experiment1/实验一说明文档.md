# 实验一 说明文档



## 实验要求

**亚运会数据统计[Python/Nodejs]**

统计亚运会关注人数最多的比赛



## 实验环境

- 操作系统：Linux
- 虚拟机：VMWare
- 编程语言：Nodejs 18.17.0



## 执行

1. 在code文件夹下打开终端
2. 输入 **npm run dev**
3. 爬虫时间在7-8分钟左右，输出在code/outputs/result.json



## 依赖

```json
{
"dependencies": {
    "express": "^4.18.2",
    "generic-pool": "^3.9.0",
    "node-schedule": "^2.1.1",
    "puppeteer": "^21.3.6"
  }
}
```

说明：

express用于创建Web服务器

generic-pool用于创建连接池，做性能优化

node-schedule用于开启任务进行自动爬虫--比如每晚十二点进行自动爬虫

puppeteer用于爬虫



## 爬虫思路

1. 首先进入 https://tiyu.baidu.com/major/home/%E6%9D%AD%E5%B7%9E%E4%BA%9A%E8%BF%90%E4%BC%9A/tab/%E8%B5%9B%E7%A8%8B/date/2023-09-19 该页面，这是2023-9-19日比赛详情的页面，通过点击更多和向下不断滚轮可以获取到9-19到10-8的所有比赛信息。
2. 使用puppeteer库模拟浏览器点击行为和滚轮行为，使得所有异步数据全部完毕，开始爬取该页面下所有的.wa-olympic-schedule-list-item类a标签身上的href属性，该属性的url地址指向了直播间的地址。同时为了确保能够区分每场比赛属于哪一天，我们还需爬取该页面下的.date和.list-num，这两个标签分别为比赛日期和该日比赛场次的类标签。
3. 接下来需要对上一步爬取的href进行拼接，前缀加上https，后缀由/数据改为/聊天室，这样能够确保加载完成聊天室人数。
4. 爬取每个直播间的.text-position-talkroom标签拿到直播间人数。
5. 对数据进行统计，找到每日人数最多的比赛，并通过fs模块导出为json。



## 实现细节

### 性能优化

本次实验需要爬取400+页面，要花费较长的时间，需要进行性能优化

我对性能优化有以下两个思路

1. 使用Promise.all 进行处理，但需要对Promise.all进行进一步的封装，进行并发限制
2. 对puppeteer进行优化

我选择了第二个方案，我的优化步骤如下

1. 使用generic-pool连接池技术进行优化，避免过度的开关浏览器
2. 在单页面进行多url跳转而不是频繁开关浏览器浪费时间

### 自动化爬取

在实验中有一个要求即在指定时间自动进行爬取

实现步骤如下

1. 为了达到自动化爬取，首先要确保程序不能停止，于是借助express框架开启了web服务器
2. 借助node-schedule库开启任务，在指定时间进行任务执行，可在code/src/setting.js中修改爬虫的时间



## 核心代码

### 封装puppeteer库

```js
const puppeteer = require('puppeteer');
const browserPool = require('./browser-pool');


/**
 * 滚动到底部
 * @param {} page 
 */
async function autoScroll(page) {
    console.log('正在滚动页面,时间较长请稍等...')
    await page.evaluate(async () => {
        await new Promise((resolve) => {
            let totalHeight = 0;
            const distance = 100;
            const scrollInterval = setInterval(() => {
                const scrollHeight = document.body.scrollHeight;
                window.scrollBy(0, distance);
                totalHeight += distance;
                if (totalHeight >= scrollHeight) {
                    clearInterval(scrollInterval);
                    console.log('滚动页面结束...')
                    resolve();
                }
            }, 100);
        });
    });
}


/**
 * 优化
 * 使用链接池
 */
async function runPuppeteerScriptForTexts(urls, classTag) {
    // console.log(`开始爬取${url}的Text`)
    return new Promise(async (resolve) => {
        const browser = await browserPool.acquire(); // 从连接池获取浏览器实例

        const page = await browser.newPage();
        await page.setDefaultNavigationTimeout(60000);

        const dynamicContents = []

        for (const urlItem in urls) {
            console.log(`开始爬取${urls[urlItem]}的Text`)
            await page.goto(urls[urlItem]);
            await page.waitForSelector(classTag);
            const dynamicContent = await page.evaluate((classTag) => {
                const element = document.querySelector(classTag);
                return element ? element.textContent : null;
            }, classTag);
            dynamicContents.push(dynamicContent)
            console.log(`结束爬取${urls[urlItem]}的Text`)
        }

        // 关闭页面
        await page.close();
    
        // 将浏览器实例返回连接池
        browserPool.release(browser);

        resolve(
            dynamicContents
        )
    })
}


async function runPuppeteerScript(url, classTag, classTagText, classTagDate, classTagName) {
    console.log(`开始爬取${url}的Text和href`)
    return new Promise(async (resolve) => {
        const browser = await browserPool.acquire(); // 从连接池获取浏览器实例

        const page = await browser.newPage();
        await page.goto(url);
        await page.click('.btn-load.m-c-bg-color-white.m-c-color-gray.btn-middle');
        // 模拟滚动到底部
        await autoScroll(page);
        await page.waitForSelector(classTag);

        // 执行其他 Puppeteer 操作...
        const dynamicHrefs = await page.evaluate((classTag) => {
            const elements = document.querySelectorAll(classTag);
            const data = []
            elements.forEach(element => {
                data.push(element.getAttribute('href'))
            })
            return data
        }, classTag);

        const dynamicContents = await page.evaluate((classTagText) => {
            const elements = document.querySelectorAll(classTagText);
            const data = []
            elements.forEach(element => {
                data.push(element.textContent)
            })
            return data
        }, classTagText)

        const dynamicDates = await page.evaluate((classTagDate) => {
            const elements = document.querySelectorAll(classTagDate);
            console.log(elements)
            const data = []
            elements.forEach(element => {
                data.push(element.textContent)
            })
            return data
        }, classTagDate)

        const dynamicNames = await page.evaluate((classTagName) => {
            const elements = document.querySelectorAll(classTagName);
            console.log(elements)
            const data = []
            elements.forEach(element => {
                data.push(element.textContent)
            })
            return data
        }, classTagName)

        // 关闭页面
        await page.close();
        console.log(`结束爬取${url}的Text和Href`)

        // 将浏览器实例返回连接池
        browserPool.release(browser);

        resolve({
            dynamicHrefs,
            dynamicContents,
            dynamicDates,
            dynamicNames
        })
    })
}


module.exports = {
    runPuppeteerScript,
    runPuppeteerScriptForTexts
}
```

### 爬虫代码

```js
const api = require('./api')
const myPuppeteer = require('./utils/puppeteer')
const tagClasses = require('./tagClasses')
const convertToNumber = require('./methods/convertToNumber')
const findMaxIndexes = require('./methods/findMaxIndexes')
const output = require('./utils/file-output')

async function getAllHrefsAndText() {
    const res = await myPuppeteer.runPuppeteerScript(api.API_ROOT, tagClasses.liveHrefTag, tagClasses.totalCom, tagClasses.dateTag, tagClasses.competitionName)
    return res
}

function restructData(data) {

    const res = {}

    const gamesCount = data.dynamicContents.map(item => convertToNumber.convertComToNumber(item))

    let indexDate = 0
    let indexHrefs = 0
    gamesCount.forEach(item => {
        const date = data.dynamicDates[indexDate]
        const gameCount = data.dynamicContents[indexDate]
        indexDate++
        const url = data.dynamicHrefs.slice(indexHrefs, indexHrefs + item).map(urlItem => {
            return 'https:' + urlItem.slice(0, urlItem.indexOf('/tab')) + '/tab/' + api.API_LIVE_SUFFIEX
        })
        const names = data.dynamicNames.slice(indexHrefs, indexHrefs + item)
        indexHrefs += item
        const obj = {
            date,
            gameCount,
            url,
            names
        }
        res[date] = obj
    })

    return res
}

async function getLivePeople(data) {

    for (const item in data) {
        // const peoCounts = []
        // for (const urlItem of data[item].url) {
        //     const peoCount = await myPuppeteer.runPuppeteerScriptForTexts(urlItem, tagClasses.peopleTag) 
        //     peoCounts.push(peoCount)
        // }
        const peoCounts = await myPuppeteer.runPuppeteerScriptForTexts(data[item].url, tagClasses.peopleTag)
        data[item].peoCounts = peoCounts
    }

    return data
}


function reconstructDataForPeople(data) {
    for (const item in data) {
        const pCounts = data[item].peoCounts
        const pStr = pCounts.join(" ") 
        const regexPeople = /\((.*?)\)/g // 匹配括号内的内容
        const matchPeople = []
        let match
        while ((match = regexPeople.exec(pStr)) !== null) {
            matchPeople.push(match[1]);
        }
        data[item].peoCounts = matchPeople
    }
    return data
}

/**
 * 获取输出的data内容
 * @param {Array} data 
 */
function getOutputData(data) {
    const res = {}

    for (const item in data) {
        const objItem = {}

        objItem.date = '比赛的日期为:' + item
        objItem.mostPopular = []

        data[item].mostIndexes.map(index => {
            const liveObj = {
                'title': '人气王之一!',
                'name': data[item].names[index],
                'count': '人数为' + data[item].peoCounts[index]
            }
            objItem.mostPopular.push(liveObj)
        })

        res[item] = objItem
    }

    return res
}


async function reptile() {
    let data = reconstructDataForPeople(await getLivePeople(restructData(await getAllHrefsAndText())))

    for (const item in data) {
        // 字符串to数值
        const numberCount = data[item].peoCounts.map(count => {
            return convertToNumber.convertToNumber(count)
        })
        const mostIndexes = findMaxIndexes(numberCount)
        data[item].mostIndexes = mostIndexes
    }

    // 格式化data
    data = getOutputData(data)

    // 输出
    output.outputDataToJSON(data)

    return data
}


module.exports = reptile
```



## 实验心得

本次实验对我一个web服务端小白来讲是一个巨大的挑战，但同时也是进入大学之后收获最多的一次实验，虽然之前对Node.js有过比较系统的学习，但并没有真正用Node去做过服务端的开发，这是收获之一。在实验的过程中遇到了种种问题，比如如何能够爬取到每天的比赛数据、如何爬取异步数据、如何让爬虫快一点等等，在经过不断的试错之后最终达到一个较理想的状态，这是一个很大的收获。我很期待接下来的挑战！